{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "\n",
    "matplotlib.style.use('ggplot') # Look Pretty\n",
    "\n",
    "\n",
    "# Leave this alone until indicated:\n",
    "Test_PCA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Convenience Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is for your visualization convenience only. You aren't expected to know how to put this together yourself, although you should be able to follow the code by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model, X, y):\n",
    "    print(\"Plotting...\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    padding = 0.1\n",
    "    resolution = 0.1\n",
    "\n",
    "    #(2 for benign, 4 for malignant)\n",
    "    colors = {2:'royalblue', 4:'lightsalmon'} \n",
    "\n",
    "\n",
    "    # Calculate the boundaries\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_min -= x_range * padding\n",
    "    y_min -= y_range * padding\n",
    "    x_max += x_range * padding\n",
    "    y_max += y_range * padding\n",
    "\n",
    "    # Create a 2D Grid Matrix. The values stored in the matrix\n",
    "    # are the predictions of the class at at said location\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),np.arange(y_min, y_max, resolution))\n",
    "\n",
    "    # What class does the classifier say?\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour map\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.seismic)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Plot your testing points as well...\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)\n",
    "        plt.scatter(X[indices, 0], X[indices, 1], c=colors[label], alpha=0.8)\n",
    "\n",
    "    p = model.get_params()\n",
    "    plt.title('K = ' + str(p['n_neighbors']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load in the dataset, identify nans, and set proper headers. Be sure to verify the rows line up by looking at the file in a text editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>thickness</th>\n",
       "      <th>size</th>\n",
       "      <th>shape</th>\n",
       "      <th>adhesion</th>\n",
       "      <th>epithelial</th>\n",
       "      <th>nuclei</th>\n",
       "      <th>chromatin</th>\n",
       "      <th>nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017122</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample  thickness  size  shape  adhesion  epithelial  nuclei  chromatin  \\\n",
       "0  1002945          5     4      4         5           7    10.0          3   \n",
       "1  1015425          3     1      1         1           2     2.0          3   \n",
       "2  1016277          6     8      8         1           3     4.0          3   \n",
       "3  1017023          4     1      1         3           2     1.0          3   \n",
       "4  1017122          8    10     10         8           7    10.0          9   \n",
       "\n",
       "   nucleoli  mitoses  status  \n",
       "0         2        1       2  \n",
       "1         1        1       2  \n",
       "2         7        1       2  \n",
       "3         1        1       2  \n",
       "4         7        1       4  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Datasets/breast-cancer-wisconsin.data\", na_values = \"?\")\n",
    "df.columns = ['sample', 'thickness', 'size', 'shape', 'adhesion', 'epithelial', 'nuclei', 'chromatin', 'nucleoli', 'mitoses', 'status']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.,   2.,   4.,   1.,   3.,   9.,   7.,  nan,   5.,   8.,   6.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"nuclei\"].unique()# had ? converted to nan in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>thickness</th>\n",
       "      <th>size</th>\n",
       "      <th>shape</th>\n",
       "      <th>adhesion</th>\n",
       "      <th>epithelial</th>\n",
       "      <th>nuclei</th>\n",
       "      <th>chromatin</th>\n",
       "      <th>nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.980000e+02</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.071807e+06</td>\n",
       "      <td>4.416905</td>\n",
       "      <td>3.137536</td>\n",
       "      <td>3.210602</td>\n",
       "      <td>2.809456</td>\n",
       "      <td>3.217765</td>\n",
       "      <td>3.548387</td>\n",
       "      <td>3.438395</td>\n",
       "      <td>2.869628</td>\n",
       "      <td>1.590258</td>\n",
       "      <td>2.690544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.175323e+05</td>\n",
       "      <td>2.817673</td>\n",
       "      <td>3.052575</td>\n",
       "      <td>2.972867</td>\n",
       "      <td>2.856606</td>\n",
       "      <td>2.215408</td>\n",
       "      <td>3.645226</td>\n",
       "      <td>2.440056</td>\n",
       "      <td>3.055004</td>\n",
       "      <td>1.716162</td>\n",
       "      <td>0.951596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.163400e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.702582e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.171710e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.238354e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.345435e+07</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sample   thickness        size       shape    adhesion  \\\n",
       "count  6.980000e+02  698.000000  698.000000  698.000000  698.000000   \n",
       "mean   1.071807e+06    4.416905    3.137536    3.210602    2.809456   \n",
       "std    6.175323e+05    2.817673    3.052575    2.972867    2.856606   \n",
       "min    6.163400e+04    1.000000    1.000000    1.000000    1.000000   \n",
       "25%    8.702582e+05    2.000000    1.000000    1.000000    1.000000   \n",
       "50%    1.171710e+06    4.000000    1.000000    1.000000    1.000000   \n",
       "75%    1.238354e+06    6.000000    5.000000    5.000000    4.000000   \n",
       "max    1.345435e+07   10.000000   10.000000   10.000000   10.000000   \n",
       "\n",
       "       epithelial      nuclei   chromatin    nucleoli     mitoses      status  \n",
       "count  698.000000  682.000000  698.000000  698.000000  698.000000  698.000000  \n",
       "mean     3.217765    3.548387    3.438395    2.869628    1.590258    2.690544  \n",
       "std      2.215408    3.645226    2.440056    3.055004    1.716162    0.951596  \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    2.000000  \n",
       "25%      2.000000    1.000000    2.000000    1.000000    1.000000    2.000000  \n",
       "50%      2.000000    1.000000    3.000000    1.000000    1.000000    2.000000  \n",
       "75%      4.000000    6.000000    5.000000    4.000000    1.000000    4.000000  \n",
       "max     10.000000   10.000000   10.000000   10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(698, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy out the status column into a slice, then drop it from the main dataframe. Always verify you properly executed the drop by double checking (printing out the resulting operating)! Many people forget to set the right axis here.\n",
    "\n",
    "If you goofed up on loading the dataset and notice you have a `sample` column, this would be a good place to drop that too if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(698, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"status\"]\n",
    "df.drop(labels = [\"sample\",\"status\"], inplace = True, axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(698,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the labels safely extracted from the dataset, replace any nan values with the mean feature / column value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(np.around(df.mean(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nuclei = pd.to_numeric(df.nuclei, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.,   2.,   4.,   1.,   3.,   9.,   7.,   5.,   8.,   6.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nuclei.unique()# had nan in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do train_test_split. Use the same variable names as on the EdX platform in the reading material, but set the random_state=7 for reproducibility, and keep the test_size at 0.5 (50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(df, labels, test_size = 0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the basic SKLearn preprocessing scalers. We know that the features consist of different units mixed in together, so it might be reasonable to assume feature scaling is necessary. Print out a description of the dataset, post transformation. Recall: when you do pre-processing, which portion of the dataset is your model trained upon? Also which portion(s) of your dataset actually get transformed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#T = preprocessing.StandardScaler().fit_transform(df)\n",
    "#T = preprocessing.MinMaxScaler().fit_transform(df)\n",
    "#T = preprocessing.MaxAbsScaler().fit_transform(df)\n",
    "#T = preprocessing.Normalizer().fit_transform(df)\n",
    "#T = preprocessing.RobustScaler().fit_tranform(df)\n",
    "#T = df # No Change\n",
    "\n",
    "T = preprocessing.Normalizer().fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Train = T.transform(data_train)\n",
    "data_Test = T.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and Isomap are your new best friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 2D Isomap Manifold\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "\n",
    "if Test_PCA:\n",
    "    print('Computing 2D Principle Components')\n",
    "    # TODO: Implement PCA here. Save your model into the variable 'model'.\n",
    "    # You should reduce down to two dimensions.\n",
    "    \n",
    "    model = PCA(n_components = 2, svd_solver = \"Full\")\n",
    "\n",
    "else:\n",
    "    print('Computing 2D Isomap Manifold')\n",
    "    # TODO: Implement Isomap here. Save your model into the variable 'model'\n",
    "    # Experiment with K values from 5-10.\n",
    "    # You should reduce down to two dimensions.\n",
    "\n",
    "    model = manifold.Isomap(n_neighbors = 5, n_components = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model against data_train, then transform both `data_train` and `data_test` using your model. You can save the results right back into the variables themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(data_train)\n",
    "data_train = model.transform(data_train)\n",
    "data_test = model.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and train `KNeighborsClassifier` on your projected 2D training data here. You can name your variable `knmodel`. You can use any `K` value from 1 - 15, so play around with it and see what results you can come up. Your goal is to find a good balance where you aren't too specific (low-K), nor are you too general (high-K). You should also experiment with how changing the weights parameter affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "0.954154727794\n",
      "K = 2\n",
      "0.936962750716\n",
      "K = 3\n",
      "0.968481375358\n",
      "K = 4\n",
      "0.962750716332\n",
      "K = 5\n",
      "0.974212034384\n",
      "K = 6\n",
      "0.959885386819\n",
      "K = 7\n",
      "0.968481375358\n",
      "K = 8\n",
      "0.965616045845\n",
      "K = 9\n",
      "0.971346704871\n",
      "K = 10\n",
      "0.965616045845\n",
      "K = 11\n",
      "0.971346704871\n",
      "K = 12\n",
      "0.968481375358\n",
      "K = 13\n",
      "0.971346704871\n",
      "K = 14\n",
      "0.971346704871\n",
      "K = 15\n",
      "0.971346704871\n"
     ]
    }
   ],
   "source": [
    "x = list()\n",
    "for i in range(1,16):\n",
    "    knmodel = KNeighborsClassifier(n_neighbors = i)\n",
    "    knmodel.fit(data_train, label_train)\n",
    "    print(\"K = \" + str(i))\n",
    "    print(knmodel.score(data_test, label_test))\n",
    "    x.append(knmodel.score(data_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.95415472779369626,\n",
       " 0.93696275071633239,\n",
       " 0.96848137535816614,\n",
       " 0.96275071633237819,\n",
       " 0.97421203438395421,\n",
       " 0.95988538681948421,\n",
       " 0.96848137535816614,\n",
       " 0.96561604584527216,\n",
       " 0.97134670487106012,\n",
       " 0.96561604584527216,\n",
       " 0.97134670487106012,\n",
       " 0.96848137535816614,\n",
       " 0.97134670487106012,\n",
       " 0.97134670487106012,\n",
       " 0.97134670487106012]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to always keep the domain of the problem in mind! It's WAY more important to errantly classify a benign tumor as malignant, and have it removed, than to incorrectly leave a malignant tumor, believing it to be benign, and then having the patient progress in cancer. Since the UDF weights don't give you any class information, the only way to introduce this data into SKLearn's KNN Classifier is by \"baking\" it into your data. For example, randomly reducing the ratio of benign samples compared to malignant samples from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate and display the accuracy of the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaxAccuracyAtKVal = x.index(max(x)) + 1\n",
    "MaxAccuracyAtKVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97421203438395421"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knmodel = KNeighborsClassifier(n_neighbors = MaxAccuracyAtKVal)\n",
    "knmodel.fit(data_train, label_train)\n",
    "knmodel.score(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AOAIQg+ABiC4AOAIf4NhPObPmHgVP0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24136cc27b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEJCAYAAAB4yveGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGu1LFewAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x241370d92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotDecisionBoundary(knmodel, data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No Preprocessing: 0.968571428571\n",
    "#MaxAbsScaler(): 0.98095238095238091\n",
    "#MinMaxScaler(): 0.98095238095238091\n",
    "#StandardScaler():0.98095238095238091\n",
    "#RobustScaler(): 0.98095238095238091\n",
    "#Normalizer(): 0.97421203438395421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "0.961904761905\n",
      "K = 2\n",
      "0.971428571429\n",
      "K = 3\n",
      "0.971428571429\n",
      "K = 4\n",
      "0.971428571429\n",
      "K = 5\n",
      "0.980952380952\n",
      "K = 6\n",
      "0.980952380952\n",
      "K = 7\n",
      "0.980952380952\n",
      "K = 8\n",
      "0.980952380952\n",
      "K = 9\n",
      "0.980952380952\n",
      "K = 10\n",
      "0.980952380952\n",
      "K = 11\n",
      "0.980952380952\n",
      "K = 12\n",
      "0.980952380952\n",
      "K = 13\n",
      "0.980952380952\n",
      "K = 14\n",
      "0.980952380952\n",
      "K = 15\n",
      "0.980952380952\n",
      "K = 16\n",
      "0.980952380952\n",
      "K = 17\n",
      "0.980952380952\n",
      "K = 18\n",
      "0.980952380952\n",
      "K = 19\n",
      "0.980952380952\n",
      "K = 20\n",
      "0.980952380952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98095238095238091"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(df, labels, test_size = 0.15, random_state = 7)\n",
    "\n",
    "T = preprocessing.MinMaxScaler().fit(data_train, label_train)\n",
    "\n",
    "data_Train = T.transform(data_train)\n",
    "data_Test = T.transform(data_test)\n",
    "\n",
    "if Test_PCA:\n",
    "    # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 principal components! A lot of information\n",
    "    # (variance) is lost during the process, as I'm sure you can imagine. But\n",
    "    # you have to drop the dimension down to two, otherwise you wouldn't be able\n",
    "    # to visualize a 2D decision surface / boundary. In the wild, you'd probably\n",
    "    # leave in a lot more dimensions, which is better for higher accuracy, but\n",
    "    # worse for visualizing the decision boundary;\n",
    "    #\n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "\n",
    "    # TODO: Implement PCA here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2, svd_solver = \"full\")\n",
    "    pca.fit(data_train)\n",
    "    data_train = pca.transform(data_train)\n",
    "    data_test = pca.transform(data_test)\n",
    "\n",
    "else:\n",
    "    # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 components! A lot of information has been is\n",
    "    # lost during the process, as I'm sure you can imagine. But if you have\n",
    "    # non-linear data that can be represented on a 2D manifold, you probably will\n",
    "    # be left with a far superior dataset to use for classification. Plus by\n",
    "    # having the images in 2D space, you can plot them as well as visualize a 2D\n",
    "    # decision surface / boundary. In the wild, you'd probably leave in a lot more\n",
    "    # dimensions, which is better for higher accuracy, but worse for visualizing the\n",
    "    # decision boundary;\n",
    "    \n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "    \n",
    "    # TODO: Implement Isomap here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn import manifold\n",
    "    iso = manifold.Isomap(n_components = 2)\n",
    "    iso.fit(data_train)\n",
    "    data_train = iso.transform(data_train)\n",
    "    data_test = iso.transform(data_test)\n",
    "    \n",
    "x = list()\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(data_train, label_train)\n",
    "    print(\"K = \" + str(i))\n",
    "    print(knn.score(data_test, label_test))\n",
    "    x.append(knn.score(data_test, label_test))\n",
    "    \n",
    "MaxAccuracyAtKVal = x.index(max(x)) + 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = MaxAccuracyAtKVal)\n",
    "knn.fit(data_train, label_train)\n",
    "knn.score(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "0.961904761905\n",
      "K = 2\n",
      "0.971428571429\n",
      "K = 3\n",
      "0.971428571429\n",
      "K = 4\n",
      "0.971428571429\n",
      "K = 5\n",
      "0.980952380952\n",
      "K = 6\n",
      "0.980952380952\n",
      "K = 7\n",
      "0.980952380952\n",
      "K = 8\n",
      "0.980952380952\n",
      "K = 9\n",
      "0.980952380952\n",
      "K = 10\n",
      "0.980952380952\n",
      "K = 11\n",
      "0.980952380952\n",
      "K = 12\n",
      "0.980952380952\n",
      "K = 13\n",
      "0.980952380952\n",
      "K = 14\n",
      "0.980952380952\n",
      "K = 15\n",
      "0.980952380952\n",
      "K = 16\n",
      "0.980952380952\n",
      "K = 17\n",
      "0.980952380952\n",
      "K = 18\n",
      "0.980952380952\n",
      "K = 19\n",
      "0.980952380952\n",
      "K = 20\n",
      "0.980952380952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98095238095238091"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(df, labels, test_size = 0.15, random_state = 7)\n",
    "\n",
    "T = preprocessing.RobustScaler().fit(data_train, label_train)\n",
    "\n",
    "data_Train = T.transform(data_train)\n",
    "data_Test = T.transform(data_test)\n",
    "\n",
    "if Test_PCA:\n",
    "    # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 principal components! A lot of information\n",
    "    # (variance) is lost during the process, as I'm sure you can imagine. But\n",
    "    # you have to drop the dimension down to two, otherwise you wouldn't be able\n",
    "    # to visualize a 2D decision surface / boundary. In the wild, you'd probably\n",
    "    # leave in a lot more dimensions, which is better for higher accuracy, but\n",
    "    # worse for visualizing the decision boundary;\n",
    "    #\n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "\n",
    "    # TODO: Implement PCA here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2, svd_solver = \"full\")\n",
    "    pca.fit(data_train)\n",
    "    data_train = pca.transform(data_train)\n",
    "    data_test = pca.transform(data_test)\n",
    "\n",
    "else:\n",
    "    # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 components! A lot of information has been is\n",
    "    # lost during the process, as I'm sure you can imagine. But if you have\n",
    "    # non-linear data that can be represented on a 2D manifold, you probably will\n",
    "    # be left with a far superior dataset to use for classification. Plus by\n",
    "    # having the images in 2D space, you can plot them as well as visualize a 2D\n",
    "    # decision surface / boundary. In the wild, you'd probably leave in a lot more\n",
    "    # dimensions, which is better for higher accuracy, but worse for visualizing the\n",
    "    # decision boundary;\n",
    "    \n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "    \n",
    "    # TODO: Implement Isomap here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn import manifold\n",
    "    iso = manifold.Isomap(n_components = 2)\n",
    "    iso.fit(data_train)\n",
    "    data_train = iso.transform(data_train)\n",
    "    data_test = iso.transform(data_test)\n",
    "    \n",
    "x = list()\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(data_train, label_train)\n",
    "    print(\"K = \" + str(i))\n",
    "    print(knn.score(data_test, label_test))\n",
    "    x.append(knn.score(data_test, label_test))\n",
    "    \n",
    "MaxAccuracyAtKVal = x.index(max(x)) + 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = MaxAccuracyAtKVal)\n",
    "knn.fit(data_train, label_train)\n",
    "knn.score(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "0.961904761905\n",
      "K = 2\n",
      "0.971428571429\n",
      "K = 3\n",
      "0.971428571429\n",
      "K = 4\n",
      "0.971428571429\n",
      "K = 5\n",
      "0.980952380952\n",
      "K = 6\n",
      "0.980952380952\n",
      "K = 7\n",
      "0.980952380952\n",
      "K = 8\n",
      "0.980952380952\n",
      "K = 9\n",
      "0.980952380952\n",
      "K = 10\n",
      "0.980952380952\n",
      "K = 11\n",
      "0.980952380952\n",
      "K = 12\n",
      "0.980952380952\n",
      "K = 13\n",
      "0.980952380952\n",
      "K = 14\n",
      "0.980952380952\n",
      "K = 15\n",
      "0.980952380952\n",
      "K = 16\n",
      "0.980952380952\n",
      "K = 17\n",
      "0.980952380952\n",
      "K = 18\n",
      "0.980952380952\n",
      "K = 19\n",
      "0.980952380952\n",
      "K = 20\n",
      "0.980952380952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98095238095238091"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(df, labels, test_size = 0.15, random_state = 7)\n",
    "\n",
    "T = preprocessing.StandardScaler().fit(data_train, label_train)\n",
    "\n",
    "data_Train = T.transform(data_train)\n",
    "data_Test = T.transform(data_test)\n",
    "\n",
    "if Test_PCA:\n",
    "    # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 principal components! A lot of information\n",
    "    # (variance) is lost during the process, as I'm sure you can imagine. But\n",
    "    # you have to drop the dimension down to two, otherwise you wouldn't be able\n",
    "    # to visualize a 2D decision surface / boundary. In the wild, you'd probably\n",
    "    # leave in a lot more dimensions, which is better for higher accuracy, but\n",
    "    # worse for visualizing the decision boundary;\n",
    "    #\n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "\n",
    "    # TODO: Implement PCA here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2, svd_solver = \"full\")\n",
    "    pca.fit(data_train)\n",
    "    data_train = pca.transform(data_train)\n",
    "    data_test = pca.transform(data_test)\n",
    "\n",
    "else:\n",
    "    # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 components! A lot of information has been is\n",
    "    # lost during the process, as I'm sure you can imagine. But if you have\n",
    "    # non-linear data that can be represented on a 2D manifold, you probably will\n",
    "    # be left with a far superior dataset to use for classification. Plus by\n",
    "    # having the images in 2D space, you can plot them as well as visualize a 2D\n",
    "    # decision surface / boundary. In the wild, you'd probably leave in a lot more\n",
    "    # dimensions, which is better for higher accuracy, but worse for visualizing the\n",
    "    # decision boundary;\n",
    "    \n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "    \n",
    "    # TODO: Implement Isomap here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn import manifold\n",
    "    iso = manifold.Isomap(n_components = 2)\n",
    "    iso.fit(data_train)\n",
    "    data_train = iso.transform(data_train)\n",
    "    data_test = iso.transform(data_test)\n",
    "    \n",
    "x = list()\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(data_train, label_train)\n",
    "    print(\"K = \" + str(i))\n",
    "    print(knn.score(data_test, label_test))\n",
    "    x.append(knn.score(data_test, label_test))\n",
    "    \n",
    "MaxAccuracyAtKVal = x.index(max(x)) + 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = MaxAccuracyAtKVal)\n",
    "knn.fit(data_train, label_train)\n",
    "knn.score(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "0.961904761905\n",
      "K = 2\n",
      "0.971428571429\n",
      "K = 3\n",
      "0.971428571429\n",
      "K = 4\n",
      "0.971428571429\n",
      "K = 5\n",
      "0.980952380952\n",
      "K = 6\n",
      "0.980952380952\n",
      "K = 7\n",
      "0.980952380952\n",
      "K = 8\n",
      "0.980952380952\n",
      "K = 9\n",
      "0.980952380952\n",
      "K = 10\n",
      "0.980952380952\n",
      "K = 11\n",
      "0.980952380952\n",
      "K = 12\n",
      "0.980952380952\n",
      "K = 13\n",
      "0.980952380952\n",
      "K = 14\n",
      "0.980952380952\n",
      "K = 15\n",
      "0.980952380952\n",
      "K = 16\n",
      "0.980952380952\n",
      "K = 17\n",
      "0.980952380952\n",
      "K = 18\n",
      "0.980952380952\n",
      "K = 19\n",
      "0.980952380952\n",
      "K = 20\n",
      "0.980952380952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98095238095238091"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(df, labels, test_size = 0.15, random_state = 7)\n",
    "\n",
    "T = preprocessing.MaxAbsScaler().fit(data_train, label_train)\n",
    "\n",
    "data_Train = T.transform(data_train)\n",
    "data_Test = T.transform(data_test)\n",
    "\n",
    "if Test_PCA:\n",
    "    # INFO: PCA is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 principal components! A lot of information\n",
    "    # (variance) is lost during the process, as I'm sure you can imagine. But\n",
    "    # you have to drop the dimension down to two, otherwise you wouldn't be able\n",
    "    # to visualize a 2D decision surface / boundary. In the wild, you'd probably\n",
    "    # leave in a lot more dimensions, which is better for higher accuracy, but\n",
    "    # worse for visualizing the decision boundary;\n",
    "    #\n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "\n",
    "    # TODO: Implement PCA here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = 2, svd_solver = \"full\")\n",
    "    pca.fit(data_train)\n",
    "    data_train = pca.transform(data_train)\n",
    "    data_test = pca.transform(data_test)\n",
    "\n",
    "else:\n",
    "    # INFO: Isomap is used *before* KNeighbors to simplify your high dimensionality\n",
    "    # image samples down to just 2 components! A lot of information has been is\n",
    "    # lost during the process, as I'm sure you can imagine. But if you have\n",
    "    # non-linear data that can be represented on a 2D manifold, you probably will\n",
    "    # be left with a far superior dataset to use for classification. Plus by\n",
    "    # having the images in 2D space, you can plot them as well as visualize a 2D\n",
    "    # decision surface / boundary. In the wild, you'd probably leave in a lot more\n",
    "    # dimensions, which is better for higher accuracy, but worse for visualizing the\n",
    "    # decision boundary;\n",
    "    \n",
    "    # Your model should only be trained (fit) against the training data (data_train)\n",
    "    # Once you've done this, you need use the model to transform both data_train\n",
    "    # and data_test from their original high-D image feature space, down to 2D\n",
    "\n",
    "    \n",
    "    # TODO: Implement Isomap here. ONLY train against your training data, but\n",
    "    # transform both your training + test data, storing the results back into\n",
    "    # data_train, and data_test.\n",
    "    \n",
    "    from sklearn import manifold\n",
    "    iso = manifold.Isomap(n_components = 2)\n",
    "    iso.fit(data_train)\n",
    "    data_train = iso.transform(data_train)\n",
    "    data_test = iso.transform(data_test)\n",
    "    \n",
    "x = list()\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(data_train, label_train)\n",
    "    print(\"K = \" + str(i))\n",
    "    print(knn.score(data_test, label_test))\n",
    "    x.append(knn.score(data_test, label_test))\n",
    "    \n",
    "MaxAccuracyAtKVal = x.index(max(x)) + 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = MaxAccuracyAtKVal)\n",
    "knn.fit(data_train, label_train)\n",
    "knn.score(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
